#!/usr/bin/env bash

#SBATCH --ntasks={{ hpctests_computes.stdout_lines | length }}
#SBATCH --ntasks-per-node=1
#SBATCH --output=%x.out
#SBATCH --error=%x.out
#SBATCH --exclusive
#SBATCH --partition={{ hpctests_partition }}
{%if hpctests_nodes is defined %}#SBATCH --nodelist={{ hpctests_nodes }}{% endif %}

export UCX_NET_DEVICES={{ hpctests_ucx_net_devices }}
echo SLURM_JOB_NODELIST: $SLURM_JOB_NODELIST
echo SLURM_JOB_ID: $SLURM_JOB_ID
echo UCX_NET_DEVICES: $UCX_NET_DEVICES
{{ hpctests_pre_cmd }}
module load {{ hpctests_pingmatrix_modules | join(' ' ) }}

mpicc -o nxnlatbw mpi_nxnlatbw.c

# mpirun flags force using UCX TCP transports, overriding higher
# priority of OpenMPI btl/openib component, which is also using RDMA
# https://wiki.stackhpc.com/s/985dae84-7bd8-4924-94b7-9629a7827100 
mpirun -mca pml_ucx_tls any -mca pml_ucx_devices any nxnlatbw
