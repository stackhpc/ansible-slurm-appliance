#!/usr/bin/env bash

#SBATCH --ntasks=2
#SBATCH --ntasks-per-node=1
#SBATCH --output=%x.out
#SBATCH --error=%x.out
#SBATCH --exclusive
#SBATCH --partition={{ hpctests_partition }}
{%if hpctests_nodes is defined %}#SBATCH --nodelist={{ hpctests_nodes }}{% endif %}

export UCX_NET_DEVICES={{ hpctests_ucx_net_devices }}
echo SLURM_JOB_NODELIST: $SLURM_JOB_NODELIST
echo SLURM_JOB_ID: $SLURM_JOB_ID
echo UCX_NET_DEVICES: $UCX_NET_DEVICES
{{ hpctests_pre_cmd }}
module load {{ hpctests_pingpong_modules | join(' ' ) }}

#srun --mpi=pmi2 IMB-MPI1 pingpong # doesn't work in ohpc v2.1

# mpirun flags force using UCX TCP transports, overriding higher 
# priority of OpenMPI btl/openib component, which is also using RDMA 
# https://wiki.stackhpc.com/s/985dae84-7bd8-4924-94b7-9629a7827100
mpirun -mca pml_ucx_tls any -mca pml_ucx_devices any IMB-MPI1 pingpong
