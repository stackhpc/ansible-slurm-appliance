---

- name: Compute node initialisation
  hosts: localhost
  become: yes
  vars:
    os_metadata: "{{ lookup('url', 'http://169.254.169.254/openstack/latest/meta_data.json') | from_json }}"
    server_node_ip: "{{ os_metadata.meta.control_address }}"
    enable_compute: "{{ os_metadata.meta.compute | default(false) | bool }}"
    enable_resolv_conf: "{{ os_metadata.meta.resolv_conf | default(false) | bool }}"
    enable_etc_hosts: "{{ os_metadata.meta.etc_hosts | default(false) | bool }}"
    enable_cacerts: "{{ os_metadata.meta.cacerts | default(false) | bool }}"
    enable_sssd: "{{ os_metadata.meta.sssd | default(false) | bool }}"
    enable_sshd: "{{ os_metadata.meta.sshd | default(false) | bool }}"
    enable_tuned:  "{{ os_metadata.meta.tuned | default(false) | bool }}"
    enable_nfs: "{{ os_metadata.meta.nfs | default(false) | bool }}"
    enable_manila: "{{ os_metadata.meta.manila | default(false) | bool }}"
    enable_lustre: "{{ os_metadata.meta.lustre | default(false) | bool }}"
    enable_basic_users: "{{ os_metadata.meta.basic_users | default(false) | bool }}"
    enable_eessi: "{{ os_metadata.meta.eessi | default(false) | bool }}"
    enable_chrony: "{{ os_metadata.meta.chrony | default(false) | bool }}"

    # TODO: "= role defaults" - could be moved to a vars_file: on play with similar precedence effects
    resolv_conf_nameservers: []

    tuned_profile_baremetal: hpc-compute
    tuned_profile_vm: virtual-guest
    tuned_profile: "{{ tuned_profile_baremetal if ansible_virtualization_role != 'guest' else tuned_profile_vm }}"
    tuned_enabled: true
    tuned_started: true

    nfs_client_mnt_point: "/mnt"
    nfs_client_mnt_options:
    nfs_client_mnt_state: mounted
    nfs_configurations:
    nfs_enable:
      clients: false

    # openhpc: no defaults required

    os_manila_mount_shares: []
    os_manila_mount_ceph_conf_path: /etc/ceph
    os_manila_mount_state: mounted
    os_manila_mount_opts:
      - x-systemd.device-timeout=30
      - x-systemd.mount-timeout=30
      - noatime
      - _netdev # prevents mount blocking early boot before networking available
      - rw

    basic_users_groups: []
    basic_users_manage_homedir: false # homedir must already exist on shared filesystem
    basic_users_userdefaults:
      state: present
      create_home: "{{ basic_users_manage_homedir }}"
      generate_ssh_key:  "{{ basic_users_manage_homedir }}"
      ssh_key_comment: "{{ item.name }}"
    basic_users_users: []

  tasks:
    - block:
        - name: Report skipping initialization if not compute node
          # meta: end_play produces no output
          debug:
            msg: "Skipping compute initialization: Metadata enable_compute is not true"
        
        - meta: end_play
      when: not enable_compute

    - name: Ensure the mount directory exists
      file:
        path: /mnt/cluster
        state: directory
        owner: slurm
        group: root
        mode: u=rX,g=rwX,o=
    
    - name: Mount /mnt/cluster
      mount:
        path: /mnt/cluster
        src: "{{ server_node_ip }}:/exports/cluster"
        fstype: nfs
        opts: ro,sync
        state: mounted
      register: _mount_mnt_cluster
      ignore_errors: true
      # TODO: add some retries here?

    - block:
        - name: Report skipping initialization if cannot mount nfs
          # meta: end_play produces no output
          debug:
            msg: "Skipping compute initialization: Failed to mount /exports/cluster from control node {{ server_node_ip }}"
        
        - meta: end_play
      when: _mount_mnt_cluster.failed

    - name: Check if hostvars exist
      stat:
        path: "/mnt/cluster/hostvars/{{ ansible_hostname }}/hostvars.yml"
      register: hostvars_stat

    - block:
        - name: Report skipping initialization if host vars does not exist
          # meta: end_play produces no output
          debug:
            msg: "Skipping compute initialization: hostvars does not exist"

        - meta: end_play
      when: not hostvars_stat.stat.exists

    - name: Load hostvars from NFS
      # this is higher priority than vars block = normal ansible's hostvars
      include_vars:
        file: "/mnt/cluster/hostvars/{{ ansible_hostname }}/hostvars.yml" # can't use inventory_hostname

      # TODO: should /mnt/cluster now be UNMOUNTED to avoid future hang-ups?

    - name: Run chrony role
      ansible.builtin.include_role:
        name: mrlesmithjr.chrony
      when: enable_chrony | bool

    - name: Configure resolve.conf
      block:
        - name: Set nameservers in /etc/resolv.conf
          ansible.builtin.template:
            src: resolv.conf.j2
            dest: /etc/resolv.conf
            owner: root
            group: root
            mode: u=rw,og=r

        - name: Disable NetworkManager control of resolv.conf
          ansible.builtin.copy:
            src: files/NetworkManager-dns-none.conf
            dest: /etc/NetworkManager/conf.d/90-dns-none.conf
            owner: root
            group: root
            mode: u=rw,og=r
          register: _copy_nm_config

        - name: Reload NetworkManager
          ansible.builtin.systemd:
            name: NetworkManager
            state: reloaded
          when: _copy_nm_config.changed | default(false)
      when: enable_resolv_conf

    - name: Copy cluster /etc/hosts
      copy:
        src: /mnt/cluster/hosts
        dest: /etc/hosts
        owner: root
        group: root
        mode: 0644
      when: enable_etc_hosts

    - name: Configure cacerts
      ansible.builtin.include_role:
        name: cacerts
      vars:
        cacerts_cert_dir: "/mnt/cluster/cacerts"
      when: enable_cacerts

    - name: Configure sshd
      ansible.builtin.include_role:
        name: sshd
      vars:
        sshd_conf_src: "/mnt/cluster/hostconfig/{{ ansible_hostname }}/sshd.conf"
      when: enable_sshd

    - name: Configure tuned
      include_tasks: tasks/tuned.yml
      when: enable_tuned

    - name: Configure sssd
      ansible.builtin.include_role:
        name: sssd
        tasks_from: configure.yml
      vars:
        sssd_conf_src: "/mnt/cluster/hostconfig/{{ ansible_hostname }}/sssd.conf"
      when: enable_sssd

    # NFS client mount
    - name: If nfs-clients is present
      include_tasks: tasks/nfs-clients.yml
      when:
        - enable_nfs
        - nfs_enable.clients | bool or ('nfs_enable' in item and item.nfs_enable.clients | bool)
      loop: "{{ nfs_configurations }}"

    - name: Manila mounts
      block:
        - name: Read manila share info from nfs file
          include_vars:
            file: /mnt/cluster/manila_share_info.yml
          no_log: true # contains secrets
        
        - name: Ensure Ceph configuration directory exists
          ansible.builtin.file:
            path: "{{ os_manila_mount_ceph_conf_path }}"
            state: directory
            mode: "0755"
            owner: root
            group: root

        - name: Configure ceph.conf using os_manila_mount_host
          ansible.builtin.template:
            src: ceph.conf.j2
            dest: "{{ os_manila_mount_ceph_conf_path }}/ceph.conf"
            owner: root
            group: root
            mode: "0600"

        - name: Ensure mount directory exists
          ansible.builtin.file:
            path: "{{ item.mount_path }}"
            state: directory
            owner: "{{ item.mount_user | default(omit) }}"
            group: "{{ item.mount_group | default(omit) }}"
            mode: "{{ item.mount_mode | default(omit) }}"
          loop: "{{ os_manila_mount_shares }}"
          loop_control:
            label: "{{ item.share_name }}"

        - name: Write Ceph client keyring
          ansible.builtin.template:
            src: ceph.keyring.j2
            dest: "{{ os_manila_mount_ceph_conf_path }}/ceph.client.{{ item.share_user }}.keyring"
            mode: "0600"
            owner: root
            group: root
          loop: "{{ os_manila_mount_share_info }}"
          loop_control:
            label: "{{ item.share_name }}"

        - name: Mount the Ceph share
          ansible.posix.mount:
            path: "{{ item[0].mount_path }}"
            src: "{{ item[1].host }}:{{ item[1].export }}"
            fstype: ceph
            opts: "name={{ item[1].share_user }},{{ (item[0].mount_opts | default(os_manila_mount_opts)) | join(',') }}"
            # NB share_user is looked up here in case of autodetection
            state: "{{ item[0].mount_state | default(os_manila_mount_state) }}"
          loop: "{{ os_manila_mount_shares | zip(os_manila_mount_share_info) }}"
          loop_control:
            label: "{{ item[0].share_name }}"

        - name: Ensure mounted directory has correct permissions
          ansible.builtin.file:
            path: "{{ item.mount_path }}"
            state: directory
            owner: "{{ item.mount_user | default(omit) }}"
            group: "{{ item.mount_group | default(omit) }}"
            mode: "{{ item.mount_mode | default(omit) }}"
          loop: "{{ os_manila_mount_shares }}"
          loop_control:
            label: "{{ item.share_name }}"
          when: item.mount_state | default(os_manila_mount_state) in ['mounted' or 'ephemeral']
      when:
        - enable_manila
        - os_manila_mount_shares | length > 0

    - name: Configure lustre
      ansible.builtin.include_role:
        name: lustre
        tasks_from: configure.yml
      when: enable_lustre

    - name: Basic users
      block:
        - name: Create groups
          ansible.builtin.group: "{{ item }}"
          loop:  "{{ basic_users_groups }}"

        - name: Create users
          user: "{{ basic_users_userdefaults | combine(item) | filter_user_params() }}"
          loop: "{{ basic_users_users }}"
          loop_control:
            label: "{{ item.name }} [{{ item.state | default('present') }}]"
          register: basic_users_info

        - name: Write sudo rules
          blockinfile:
            path: /etc/sudoers.d/80-{{ item.name}}-user
            block: "{{ item.sudo }}"
            create: true
          loop: "{{ basic_users_users }}"
          loop_control:
            label: "{{ item.name }}"
          when: "'sudo' in item"
      when: enable_basic_users

    - name: EESSI
      block:
        - name: Copy cvmfs config
          copy:
            src: /mnt/cluster/cvmfs/default.local
            dest: /etc/cvmfs/default.local
            owner: root
            group: root
            mode: 0644

        - name: Ensure CVMFS config is setup
          command:
            cmd: "cvmfs_config setup"
      when: enable_eessi

    # NB: don't need conditional block on enable_compute as have already exited
    # if not the case
    - name: Write Munge key
      copy:
        content: "{{ openhpc_munge_key }}"
        dest: "/etc/munge/munge.key"
        owner: munge
        group: munge
        mode: 0400

    - name: Set slurmctld location for configless operation
      lineinfile:
        path: /etc/sysconfig/slurmd
        line: "SLURMD_OPTIONS='--conf-server {{ openhpc_slurm_control_host_address | default(openhpc_slurm_control_host) }}'"
        regexp: "^SLURMD_OPTIONS="
        create: yes
        owner: root
        group: root
        mode: 0644

    - name: Ensure Munge service state
      service:
        name: munge
        enabled: true
        state: started

    - name: Ensure slurmd service state
      service:
        name: slurmd
        enabled: true
        state: started

    - name: Set locked memory limits on user-facing nodes
      lineinfile:
        path: /etc/security/limits.conf
        regexp: '\* soft memlock unlimited'
        line: "* soft memlock unlimited"

    - name: Configure sshd pam module
      blockinfile:
        path: /etc/pam.d/sshd
        insertafter: 'account\s+required\s+pam_nologin.so'
        block: |
          account    sufficient   pam_access.so
          account    required     pam_slurm.so

    - name: Configure login access control
      blockinfile:
        path: /etc/security/access.conf
        block: |
          +:adm:ALL
          -:ALL:ALL

    - name: Ensure node is resumed
      # TODO: consider if this is always safe for all job states?
      command: scontrol update state=resume nodename={{ ansible_hostname }}
      register: _scontrol_update
      failed_when:
        - _scontrol_update.rc > 0
        - "'slurm_update error: Invalid node state specified' not in _scontrol_update.stderr"
