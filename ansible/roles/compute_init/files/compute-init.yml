---
- name: Compute node initialisation
  hosts: localhost
  become: true
  vars:
    os_metadata: "{{ lookup('url', 'http://169.254.169.254/openstack/latest/meta_data.json') | from_json }}"
    server_node_ip: "{{ os_metadata.meta.control_address }}"
    enable_compute: "{{ os_metadata.meta.compute | default(false) | bool }}"
    enable_resolv_conf: "{{ os_metadata.meta.resolv_conf | default(false) | bool }}"
    enable_etc_hosts: "{{ os_metadata.meta.etc_hosts | default(false) | bool }}"
    enable_cacerts: "{{ os_metadata.meta.cacerts | default(false) | bool }}"
    enable_sssd: "{{ os_metadata.meta.sssd | default(false) | bool }}"
    enable_sshd: "{{ os_metadata.meta.sshd | default(false) | bool }}"
    enable_tuned: "{{ os_metadata.meta.tuned | default(false) | bool }}"
    enable_nfs: "{{ os_metadata.meta.nfs | default(false) | bool }}"
    enable_manila: "{{ os_metadata.meta.manila | default(false) | bool }}"
    enable_lustre: "{{ os_metadata.meta.lustre | default(false) | bool }}"
    enable_basic_users: "{{ os_metadata.meta.basic_users | default(false) | bool }}"
    enable_eessi: "{{ os_metadata.meta.eessi | default(false) | bool }}"
    enable_chrony: "{{ os_metadata.meta.chrony | default(false) | bool }}"
    enable_vgpu: "{{ os_metadata.meta.vpgu | default(false) | bool }}"
    enable_nhc: "{{ os_metadata.meta.nhc | default(false) | bool }}"

    # TODO: "= role defaults" - could be moved to a vars_file: on play with similar precedence effects
    resolv_conf_nameservers: []
    tuned_profile_baremetal: hpc-compute
    tuned_profile_vm: virtual-guest
    tuned_profile: "{{ tuned_profile_baremetal if ansible_virtualization_role != 'guest' else tuned_profile_vm }}"
    tuned_enabled: true
    tuned_started: true

    nfs_enable:
      clients: false

    os_manila_mount_shares: []
    os_manila_mount_ceph_conf_path: /etc/ceph
    os_manila_mount_state: mounted
    os_manila_mount_opts:
      - x-systemd.device-timeout=30
      - x-systemd.mount-timeout=30
      - noatime
      - _netdev # prevents mount blocking early boot before networking available
      - rw
      - nodev
      - nosuid

  tasks:
    - when: not enable_compute

      block:
        - name: Report skipping initialization if not compute node
          # meta: end_play produces no output
          ansible.builtin.debug:
            msg: "Skipping compute initialization: Metadata enable_compute is not true"

        - ansible.builtin.meta: end_play
    - name: Ensure the mount directory exists
      ansible.builtin.file:
        path: /mnt/cluster
        state: directory
        owner: slurm
        group: root
        mode: u=rX,g=rwX,o=

    - name: Mount /mnt/cluster
      ansible.posix.mount:
        path: /mnt/cluster
        src: "{{ server_node_ip }}:/exports/cluster"
        fstype: nfs
        opts: ro,sync
        state: ephemeral # will be unmounted after sync, don't want it in fstab
      register: _mount_mnt_cluster
      ignore_errors: true
      # exits from playbook if this failed below, allowing ansible-init to
      # finish, which allows site.yml to continue on initial deploy

    - when: _mount_mnt_cluster.failed

      block:
        - name: Report skipping initialization if cannot mount nfs
          # meta: end_play produces no output
          ansible.builtin.debug:
            msg: "Skipping compute initialization: Failed to mount /exports/cluster from control node {{ server_node_ip }}"

        - ansible.builtin.meta: end_play
    - name: Check if hostvars exist
      become: true
      become_user: slurm
      ansible.builtin.stat:
        path: "/mnt/cluster/hostvars/{{ ansible_hostname }}/hostvars.yml"
      register: hostvars_stat

    - when: not hostvars_stat.stat.exists

      block:
        - name: Report skipping initialization if host vars does not exist
          # meta: end_play produces no output
          ansible.builtin.debug:
            msg: "Skipping compute initialization: hostvars does not exist"

        - ansible.builtin.meta: end_play
    - name: Sync /mnt/cluster to /var/tmp
      become: true
      become_user: slurm
      ansible.posix.synchronize:
        src: "/mnt/cluster/"
        dest: "/var/tmp/cluster/"
        archive: true
        recursive: true

    - name: Unmount /mnt/cluster after sync
      ansible.posix.mount:
        path: /mnt/cluster
        state: unmounted

    - name: Load hostvars
      # this is higher priority than vars block = normal ansible's hostvars
      ansible.builtin.include_vars:
        file: "/var/tmp/cluster/hostvars/{{ ansible_hostname }}/hostvars.yml"

    - name: Run chrony role
      ansible.builtin.include_role:
        name: mrlesmithjr.chrony
        tasks_from: config_chrony.yml
      vars:
        # workaround for set_facts.yml:
        chrony_config: /etc/chrony.conf
        chrony_service: chronyd
      when: enable_chrony

    - name: Configure resolve.conf
      when: enable_resolv_conf

      block:
        - name: Set nameservers in /etc/resolv.conf
          ansible.builtin.template:
            src: resolv.conf.j2
            dest: /etc/resolv.conf
            owner: root
            group: root
            mode: u=rw,og=r

        - name: Disable NetworkManager control of resolv.conf
          ansible.builtin.copy:
            src: files/NetworkManager-dns-none.conf
            dest: /etc/NetworkManager/conf.d/90-dns-none.conf
            owner: root
            group: root
            mode: u=rw,og=r
          register: _copy_nm_config

        - name: Reload NetworkManager
          ansible.builtin.systemd:
            name: NetworkManager
            state: reloaded
          when: _copy_nm_config.changed | default(false) # noqa: no-handler
    - name: Copy cluster /etc/hosts
      ansible.builtin.copy:
        src: /var/tmp/cluster/hosts
        dest: /etc/hosts
        owner: root
        group: root
        mode: "0644"
      when: enable_etc_hosts

    - name: Configure cacerts
      ansible.builtin.include_role:
        name: cacerts
      vars:
        cacerts_cert_dir: "/var/tmp/cluster/cacerts"
      when: enable_cacerts

    - name: Configure sshd
      ansible.builtin.include_role:
        name: sshd
      vars:
        sshd_conf_src: "/var/tmp/cluster/hostconfig/{{ ansible_hostname }}/sshd.conf"
      when: enable_sshd

    - name: Configure tuned
      ansible.builtin.include_tasks: tasks/tuned.yml
      when: enable_tuned

    - name: Configure sssd
      ansible.builtin.include_role:
        name: sssd
        tasks_from: configure.yml
      vars:
        sssd_conf_src: "/var/tmp/cluster/hostconfig/{{ ansible_hostname }}/sssd.conf"
      when: enable_sssd

    # NFS client mount
    - name: If nfs-clients is present
      ansible.builtin.include_role:
        name: stackhpc.nfs
        tasks_from: nfs-clients.yml
      when:
        - enable_nfs
        - nfs_enable.clients | bool or ('nfs_enable' in item and item.nfs_enable.clients | bool)
      loop: "{{ nfs_configurations }}"

    - name: Manila mounts
      when:
        - enable_manila
        - os_manila_mount_shares | length > 0

      block:
        - name: Read manila share info from nfs file
          ansible.builtin.include_vars:
            file: /var/tmp/cluster/manila_share_info.yml
          no_log: true # contains secrets

        - name: Ensure Ceph configuration directory exists
          ansible.builtin.file:
            path: "{{ os_manila_mount_ceph_conf_path }}"
            state: directory
            mode: "0755"
            owner: root
            group: root

        - name: Configure ceph.conf using os_manila_mount_host
          ansible.builtin.template:
            src: ceph.conf.j2
            dest: "{{ os_manila_mount_ceph_conf_path }}/ceph.conf"
            owner: root
            group: root
            mode: "0600"

        - name: Ensure mount directory exists
          ansible.builtin.file:
            path: "{{ item.mount_path }}"
            state: directory
            owner: "{{ item.mount_user | default(omit) }}"
            group: "{{ item.mount_group | default(omit) }}"
            mode: "{{ item.mount_mode | default(omit) }}"
          loop: "{{ os_manila_mount_shares }}"
          loop_control:
            label: "{{ item.share_name }}"

        - name: Write Ceph client keyring
          ansible.builtin.template:
            src: ceph.keyring.j2
            dest: "{{ os_manila_mount_ceph_conf_path }}/ceph.client.{{ item.share_user }}.keyring"
            mode: "0600"
            owner: root
            group: root
          loop: "{{ os_manila_mount_share_info }}"
          loop_control:
            label: "{{ item.share_name }}"

        - name: Mount the Ceph share
          ansible.posix.mount:
            path: "{{ item[0].mount_path }}"
            src: "{{ item[1].host }}:{{ item[1].export }}"
            fstype: ceph
            opts: "name={{ item[1].share_user }},{{ (item[0].mount_opts | default(os_manila_mount_opts)) | join(',') }}"
            # NB share_user is looked up here in case of autodetection
            state: "{{ item[0].mount_state | default(os_manila_mount_state) }}"
          loop: "{{ os_manila_mount_shares | zip(os_manila_mount_share_info) }}"
          loop_control:
            label: "{{ item[0].share_name }}"

        - name: Ensure mounted directory has correct permissions
          ansible.builtin.file:
            path: "{{ item.mount_path }}"
            state: directory
            owner: "{{ item.mount_user | default(omit) }}"
            group: "{{ item.mount_group | default(omit) }}"
            mode: "{{ item.mount_mode | default(omit) }}"
          loop: "{{ os_manila_mount_shares }}"
          loop_control:
            label: "{{ item.share_name }}"
          when: item.mount_state | default(os_manila_mount_state) in ['mounted' or 'ephemeral']
    - name: Configure lustre
      ansible.builtin.include_role:
        name: lustre
        tasks_from: configure.yml
      when: enable_lustre

    - name: Basic users
      ansible.builtin.include_role:
        name: basic_users
      when: enable_basic_users

    - name: EESSI
      when: enable_eessi

      # NB: don't need conditional block on enable_compute as have already exited
      # if not the case
      block:
        - name: Copy cvmfs config
          ansible.builtin.copy:
            src: /var/tmp/cluster/cvmfs/default.local
            dest: /etc/cvmfs/default.local
            owner: root
            group: root
            mode: "0644"

        - name: Ensure CVMFS config is setup # noqa: no-changed-when
          ansible.builtin.command:
            cmd: "cvmfs_config setup"

    - name: Configure VGPUs
      ansible.builtin.include_role:
        name: stackhpc.linux.vgpu
        tasks_from: 'configure.yml'
      when: enable_vgpu

    # NB: don't need conditional block on enable_compute as have already exited
    # if not the case
    - name: Write Munge key
      ansible.builtin.copy:
        # NB: openhpc_munge_key is *binary* and may not survive json encoding
        # so do same as environments/common/inventory/group_vars/all/openhpc.yml
        content: "{{ vault_openhpc_mungekey | b64decode }}"
        dest: "/etc/munge/munge.key"
        owner: munge
        group: munge
        mode: "0400"

    - name: Set slurmctld location for configless operation
      ansible.builtin.lineinfile:
        path: /etc/sysconfig/slurmd
        line: "SLURMD_OPTIONS='--conf-server {{ openhpc_slurm_control_host_address | default(openhpc_slurm_control_host) }}'"
        regexp: "^SLURMD_OPTIONS="
        create: true
        owner: root
        group: root
        mode: "0644"

    - name: Ensure Munge service state
      ansible.builtin.service:
        name: munge
        enabled: true
        state: started

    - name: Set locked memory limits on user-facing nodes
      ansible.builtin.lineinfile:
        path: /etc/security/limits.conf
        regexp: "\\* soft memlock unlimited"
        line: "* soft memlock unlimited"

    - name: Configure sshd pam module
      ansible.builtin.blockinfile:
        path: /etc/pam.d/sshd
        insertafter: "account\\s+required\\s+pam_nologin.so"
        block: |
          account    sufficient   pam_access.so
          account    required     pam_slurm.so

    - name: Configure login access control
      ansible.builtin.blockinfile:
        path: /etc/security/access.conf
        block: |
          +:adm:ALL
          -:ALL:ALL

    - name: Ensure slurmd service state
      ansible.builtin.service:
        name: slurmd
        enabled: true
        state: started

    - name: Provide NHC configuration
      ansible.builtin.include_role:
        name: nhc
        tasks_from: boot.yml
      when: enable_nhc

    - name: Ensure node is resumed # noqa: no-changed-when
      # TODO: consider if this is always safe for all job states?
      ansible.builtin.command: scontrol update state=resume nodename={{ ansible_hostname }}
      register: _scontrol_update
      failed_when:
        - _scontrol_update.rc > 0
        - "'slurm_update error: Invalid node state specified' not in _scontrol_update.stderr"
