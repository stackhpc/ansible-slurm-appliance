# NB: this only works on centos 8 / ohpc v2 as we want UCX
# TODO: add support for groups/partitions

- hosts: cluster
  name: Export/mount /opt/ohpc via NFS
  become: yes
  tasks:
    - import_role:
        name: ansible-role-cluster-nfs
      vars:
        nfs_enable:
          server:  "{{ inventory_hostname in groups['cluster_login'] | first }}"
          clients: "{{ inventory_hostname in groups['cluster_compute'] }}"
        nfs_server: "{{ hostvars[groups['cluster_login'] | first ]['server_networks']['ilab'][0] }}"
        nfs_export: "/opt/ohpc/"
        nfs_client_mnt_point: "/opt/ohpc/"

- hosts: cluster_login[0]
  name: IMB PingPong (2x scheduler-selected nodes)
  become: yes
  tags: pingpong
  vars:
    jobdir: /mnt/nfs/pingpong
  tasks:
    - name: Install gnu 9 + openmpi (w/ ucx) + performance tools
      yum:
        name: ohpc-gnu9-openmpi4-perf-tools
        state: present
    - name: Make directory
      file:
        path: "{{ jobdir }}"
        state: directory
        owner: "{{ ansible_user }}"
    - name: Create sbatch script
      copy:
        dest: "{{ jobdir }}/ping.sh"
        content: |
          #!/usr/bin/bash

          #SBATCH --ntasks=2
          #SBATCH --ntasks-per-node=1
          #SBATCH --output=%x.out
          #SBATCH --error=%x.out

          module load gnu9/9.3.0
          module load openmpi4/4.0.4
          module load imb/2019.6

          srun --mpi=pmix_v3 IMB-MPI1 pingpong
    - name: Run pingpong
      shell: sbatch --wait ping.sh
      become: no
      args:
        chdir: "{{ jobdir }}"
    - name: Slurp output
      slurp:
        src: "{{ jobdir }}/ping.sh.out"
      register: ping_out
    - debug:
        msg: "{{ ping_out['content'] | b64decode }}"

- hosts: cluster_login[0]
  name: Ping matrix (all nodes)
  become: yes
  tags: pingmatrix
  vars:
    jobdir: /mnt/nfs/pingmatrix
  tasks:
    - name: Install gnu 9 + openmpi (w/ ucx) + performance tools
      yum:
        name: ohpc-gnu9-openmpi4-perf-tools
        state: present
    - name: Make directory
      file:
        path: "{{ jobdir }}"
        state: directory
        owner: "{{ ansible_user }}"
    - name: Get info about compute nodes
      shell: sinfo -Nh
      register:
        num_compute
    - name: Download source code
      get_url:
        dest: "{{ jobdir }}/mpi_nxnlatbw.c"
        url: https://raw.githubusercontent.com/stackhpc/hpc-tests/master/apps/nxnlatbw/src/mpi_nxnlatbw.c
    - name: Compile
      shell: . /etc/profile && module load gnu9 openmpi4 && mpicc -o nxnlatbw *.c
      args:
        executable: /usr/bin/bash
        chdir: "{{ jobdir }}"
        creates: nxnlatbw
    - name: Create sbatch script
      copy:
        dest: "{{ jobdir }}/pingmatrix.sh"
        content: |
          #!/usr/bin/bash

          #SBATCH --ntasks={{ num_compute.stdout_lines | count }}
          #SBATCH --ntasks-per-node=1
          #SBATCH --output=%x.out
          #SBATCH --error=%x.out

          module load gnu9/9.3.0
          module load openmpi4/4.0.4

          srun --mpi=pmix_v3 nxnlatbw    
    - name: Run ping matrix
      shell: sbatch --wait pingmatrix.sh
      become: no
      args:
        chdir: "{{ jobdir }}"
    - name: Slurp output
      slurp:
        src: "{{ jobdir }}/pingmatrix.sh.out"
      register: ping_out
    - debug:
        msg: "{{ ping_out['content'] | b64decode }}"


- hosts: cluster_login[0]
  name: HPL (individual nodes)
  # NB: This uses the precompiled HPL provided with MKL so runs a single MPI process per node, with TBB threads added automatically by MKL to use all cores
  become: yes
  tags: hpl-solo
  vars:
    jobdir: /mnt/nfs/hpl-solo
    impi_ver: 2019.6-088
    mkl_ver: 2020.0-088
  tasks:
    - name: Install gnu 9 + openmpi (w/ ucx) + performance tools
      yum:
        name: ohpc-gnu9-openmpi4-perf-tools
        state: present
    - name: Add Intel repos
      command: yum-config-manager --add-repo https://yum.repos.intel.com/setup/intelproducts.repo
      args:
        creates: /etc/yum.repos.d/intelproducts.repo
    - name: Import Intel GPG keys
      command: rpm --import https://yum.repos.intel.com/intel-gpg-keys/GPG-PUB-KEY-INTEL-SW-PRODUCTS-2019.PUB
      # TODO: make idempotent
    - name: Install Intel MPI
      yum:
        name: "intel-mpi-{{ impi_ver }}"
    - name: Install MKL
      yum:
        name: "intel-mkl-{{ mkl_ver }}"
    - name: Make directory
      file:
        path: "{{ jobdir }}"
        state: directory
        owner: "{{ ansible_user }}"
    - name: Get info about compute nodes
      shell: sinfo -Nh
      register:
        num_compute
    - meta: end_play
    - name: Create sbatch script
      copy:
        dest: "{{ jobdir }}/hpl-solo.sh"
        content: |
          #!/usr/bin/bash

          #SBATCH --ntasks=1
          #SBATCH --output=%x.%a.out
          #SBATCH --error=%x.%a.out

          module load gnu9/9.3.0
          module load openmpi4/4.0.4
          source /opt/intel/compilers_and_libraries_{{ impi_ver | replace('-', '.') }}/linux/mpi/intel64/bin/mpivars.sh
          source /opt/intel/compilers_and_libraries_{{ mkl_ver | replace('-', '.') }}/linux/mkl/bin/mklvars.sh intel64
          export I_MPI_DEBUG=4
          #export UCX_NET_DEVICES=mlx5_0:1

          # TODO: output rank name
          mpirun -perhost 1 -np 1 --mpi=pmix_v3 $MKLROOT/benchmarks/mp_linpack/xhpl_intel64_dynamic

    - name: Run hpl
      shell: sbatch --array=0-{{ num_compute.stdout_lines | count }} --wait hpl-solo.sh
      become: no
      args:
        chdir: "{{ jobdir }}"
    - name: Slurp output
      slurp:
        src: "{{ jobdir }}/hpl-solo.sh.out"
      register: ping_out
    - debug:
        msg: "{{ ping_out['content'] | b64decode }}"
