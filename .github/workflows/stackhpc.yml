
name: Test deployment on OpenStack
on:
  workflow_dispatch:
  # push:
  #   branches:
  #     - main
  pull_request:
jobs:
  openstack:
    name: Test pre-built images on ${{ matrix.cloud }}
    strategy:
      matrix:
        cloud:
          - "arcus"   # Arcus OpenStack in rcp-cloud-portal-demo project, with RoCE
      fail-fast: false # as want clouds to continue independently
    concurrency: ${{ github.ref }} # to branch/PR
    runs-on: ubuntu-20.04
    env:
      ANSIBLE_FORCE_COLOR: True
      OS_CLOUD: openstack
      TF_VAR_cluster_name: ci${{ github.run_id }}
    steps:
      - uses: actions/checkout@v2

      - name: Setup ssh
        run: |
          set -x
          mkdir ~/.ssh
          echo "${${{ matrix.cloud }}_SSH_KEY}" > ~/.ssh/id_rsa
          chmod 0600 ~/.ssh/id_rsa
        env:
          arcus_SSH_KEY: ${{ secrets.ARCUS_SSH_KEY }}

      - name: Add bastion's ssh key to known_hosts
        run: cat environments/${{ matrix.cloud }}/bastion_fingerprint >> ~/.ssh/known_hosts
        shell: bash
      
      - name: Install ansible etc
        run: dev/setup-env.sh

      - name: Install terraform
        uses: hashicorp/setup-terraform@v1
      
      - name: Initialise terraform
        run: terraform init
        working-directory: ${{ github.workspace }}/environments/${{ matrix.cloud }}/terraform
        
      - name: Write clouds.yaml
        run: |
          mkdir -p ~/.config/openstack/
          echo "${${{ matrix.cloud }}_CLOUDS_YAML}" > ~/.config/openstack/clouds.yaml
        shell: bash
        env:
          arcus_CLOUDS_YAML: ${{ secrets.ARCUS_CLOUDS_YAML }}
      
      - name: Provision ports, inventory and other infrastructure apart from nodes
        run: |
          . venv/bin/activate
          . environments/${{ matrix.cloud }}/activate
          cd $APPLIANCES_ENVIRONMENT_ROOT/terraform
          TF_VAR_create_nodes=false terraform apply -auto-approve
      
      - name: Setup environment-specific inventory/terraform inputs
        run: |
          . venv/bin/activate
          . environments/${{ matrix.cloud }}/activate
          ansible-playbook ansible/adhoc/generate-passwords.yml
          echo vault_testuser_password: "$TESTUSER_PASSWORD" > $APPLIANCES_ENVIRONMENT_ROOT/inventory/group_vars/all/test_user.yml
          ansible-playbook ansible/adhoc/template-cloud-init.yml
        env:
          TESTUSER_PASSWORD: ${{ secrets.TEST_USER_PASSWORD }}

      - name: Provision servers
        id: provision_servers
        run: |
          . venv/bin/activate
          . environments/${{ matrix.cloud }}/activate
          cd $APPLIANCES_ENVIRONMENT_ROOT/terraform
          terraform apply -auto-approve

      - name: Get server provisioning failure messages
        id: provision_failure
        run: |
          . venv/bin/activate
          . environments/${{ matrix.cloud }}/activate
          cd $APPLIANCES_ENVIRONMENT_ROOT/terraform
          TF_FAIL_MSGS="$(../../skeleton/\{\{cookiecutter.environment\}\}/terraform/getfaults.py  $PWD)"
          echo TF failure messages: $TF_FAIL_MSGS
          echo "::set-output name=messages::${TF_FAIL_MSGS}"
        if: always() && steps.provision_servers.outcome == 'failure'
        
      - name: Delete infrastructure if failed due to lack of hosts
        run: |
          . venv/bin/activate
          . environments/${{ matrix.cloud }}/activate
          cd $APPLIANCES_ENVIRONMENT_ROOT/terraform
          terraform destroy -auto-approve
        if: ${{ always() && steps.provision_servers.outcome == 'failure' && contains(steps.provision_failure.messages, 'not enough hosts available') }}

      - name: Directly configure cluster
        run: |
          . venv/bin/activate
          . environments/${{ matrix.cloud }}/activate
          ansible all -m wait_for_connection
          ansible-playbook -v ansible/site.yml
          ansible-playbook -v ansible/ci/check_slurm.yml

      - name: Run MPI-based tests
        run: |
          . venv/bin/activate
          . environments/${{ matrix.cloud }}/activate
          ansible-playbook -vv ansible/adhoc/hpctests.yml

      - name: Confirm Open Ondemand is up (via SOCKS proxy)
        run: |
          . venv/bin/activate
          . environments/${{ matrix.cloud }}/activate
          
          # load ansible variables into shell:
          ansible-playbook ansible/ci/output_vars.yml \
            -e output_vars_hosts=openondemand \
            -e output_vars_path=$APPLIANCES_ENVIRONMENT_ROOT/vars.txt \
            -e output_vars_items=bastion_ip,bastion_user,openondemand_servername
          source $APPLIANCES_ENVIRONMENT_ROOT/vars.txt
          
          # setup ssh proxying:
          sudo apt-get --yes install proxychains
          echo proxychains installed
          ssh -v -fN -D 9050 ${bastion_user}@${bastion_ip}
          echo port 9050 forwarded

          # check OOD server returns 200:
          statuscode=$(proxychains wget \
            --quiet \
            --spider \
            --server-response \
            --no-check-certificate \
            --http-user=testuser \
            --http-password=${TESTUSER_PASSWORD} https://${openondemand_servername} \
            2>&1)
          (echo $statuscode | grep "200 OK") || (echo $statuscode  && exit 1)
        env:
          TESTUSER_PASSWORD: ${{ secrets.TEST_USER_PASSWORD }}

      # - name: Test reimage of login nodes (via rebuild adhoc)
      #   run: |
      #     . venv/bin/activate
      #     . environments/${{ matrix.cloud }}/activate
      #     ansible-playbook -v --limit login ansible/adhoc/rebuild.yml -e rebuild_image=${{ steps.packer_build.outputs.NEW_LOGIN_IMAGE_ID }}
      #     ansible login -m wait_for_connection -a 'delay=60 timeout=600' # delay allows node to go down
      #     ansible-playbook -v ansible/ci/check_slurm.yml

      # - name: Test reimage of compute nodes (via slurm)
      #   run: |
      #     . venv/bin/activate
      #     . environments/${{ matrix.cloud }}/activate
      #     ansible login -v -a "sudo scontrol reboot ASAP nextstate=RESUME reason='rebuild image:${{ steps.packer_build.outputs.NEW_COMPUTE_IMAGE_ID }}' ${TF_VAR_cluster_name}-compute-[0-3]"
      #     ansible compute -m wait_for_connection -a 'delay=60 timeout=600' # delay allows node to go down
      #     ansible-playbook -v ansible/ci/check_slurm.yml
      
      # - name: Test reimage of control node (via rebuild adhoc)
      #   run: |
      #     . venv/bin/activate
      #     . environments/${{ matrix.cloud }}/activate
      #     ansible-playbook -v --limit control ansible/adhoc/rebuild.yml -e rebuild_image=${{ steps.packer_build.outputs.NEW_CONTROL_IMAGE_ID }}
      #     ansible control -m wait_for_connection -a 'delay=60 timeout=600' # delay allows node to go down
      #     ansible-playbook ansible/slurm.yml --tags openhpc # configures partitions
      #     ansible-playbook ansible/monitoring.yml --tags prometheus # configures scrapes
      #     ansible-playbook -v ansible/ci/check_slurm.yml
      
      # - name: Check sacct state survived reimage
      #   run: |
      #     . venv/bin/activate
      #     . environments/${{ matrix.cloud }}/activate
      #     ansible-playbook -vv ansible/ci/check_sacct_hpctests.yml

      - name: Check MPI-based tests are shown in Grafana
        run: |
          . venv/bin/activate
          . environments/${{ matrix.cloud }}/activate
          ansible-playbook -vv ansible/ci/check_grafana.yml

      - name: Delete infrastructure
        run: |
          . venv/bin/activate
          . environments/${{ matrix.cloud }}/activate
          cd $APPLIANCES_ENVIRONMENT_ROOT/terraform
          terraform destroy -auto-approve
        if: ${{ success() || cancelled() }}

      # - name: Delete images
      #   run: |
      #     . venv/bin/activate
      #     . environments/${{ matrix.cloud }}/activate
      #     ansible-playbook -vv ansible/ci/delete_images.yml
