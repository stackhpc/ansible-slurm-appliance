
name: Test deployment and reimage on OpenStack
on:
  workflow_dispatch:
    inputs:
      use_RL8:
        required: true
        description: Include RL8 tests
        type: boolean
        default: false
  push:
    branches:
      - main
  pull_request:
jobs:
  openstack:
    name: openstack-ci
    concurrency: ${{ github.ref }}-{{ matrix.os_version }} # to branch/PR + OS
    runs-on: ubuntu-20.04
    strategy:
      matrix:
        os_version: [RL8, RL9]
        rl8_selected:
          - ${{ inputs.use_RL8 == true }} # only potentially true for workflow_dispatch
        rl8_branch:
          - ${{ startsWith(github.head_ref, 'rl8') == true }} # only potentially for pull_request, always false on merge
        rl8_label:
          - ${{ contains(github.event.pull_request.labels.*.name, 'RL8') }} # NB: needs a new commit if added after PR created
        exclude:
          - os_version: RL8
            rl8_selected: false
            rl8_branch: false
            rl8_label: false
    env:
      ANSIBLE_FORCE_COLOR: True
      OS_CLOUD: openstack
      CI_CLUSTER_NAME: slurmci-${{ matrix.os_version }}-${{ github.run_id }}
      CI_CLOUD: ${{ vars.CI_CLOUD }}
      ANSIBLE_INVENTORY: environments/common/inventory,environments/.caas/inventory
      DISTRO_VERSION: ${{ matrix.os_version }}
    steps:
      - uses: actions/checkout@v2

      - name: Record settings for CI cloud
        run: |
          echo CI_CLOUD: ${{ vars.CI_CLOUD }}

      - name: Setup ssh for azimuth user
        run: ssh-keygen -t rsa -q -f "$HOME/.ssh/id_rsa" -N ""

      - name: Install ansible etc
        run: dev/setup-env.sh

      - name: Write clouds.yaml
        run: |
          mkdir -p ~/.config/openstack/
          echo "${{ secrets[format('{0}_CLOUDS_YAML', vars.CI_CLOUD)] }}" > ~/.config/openstack/clouds.yaml
        shell: bash

      - name: Setup CI-specific extravars (including per-cloud vars)
        run: |
          . venv/bin/activate
          cp environments/.caas/CI-extravars.yml environments/.caas/inventory/group_vars/all/CI-extravars.yml

      - name: Provision and configure cluster, run mpi-based tests and check slurm partitions
        run: |
          . venv/bin/activate
          ansible-playbook -v ansible/site.yml
          ansible-playbook -v ansible/ci/check_slurm.yml

      # - name: Run EESSI tests
      #   run: |
      #     . venv/bin/activate
      #     ansible-playbook -vv ansible/ci/check_eessi.yml

      - name: Confirm Open Ondemand is up (via FIP)
        run: |
          . venv/bin/activate
          
          # load ansible variables into shell:
          ansible-playbook ansible/ci/output_vars.yml \
            -e output_vars_hosts=control \
            -e output_vars_path=$APPLIANCES_ENVIRONMENT_ROOT/vars.txt \
            -e output_vars_items=cluster_gateway_ip,vault_azimuth_user_password
          source $APPLIANCES_ENVIRONMENT_ROOT/vars.txt
          
          # check OOD server returns 200:
          statuscode=$(wget \
            --quiet \
            --spider \
            --server-response \
            --no-check-certificate \
            --http-user=azimuth \
            --http-password=${vault_azimuth_user_password} https://${cluster_gateway_ip} \
            2>&1)
          (echo $statuscode | grep "200 OK") || (echo $statuscode  && exit 1)

      # - name: Build environment-specific compute image
      #   id: packer_build
      #   run: |
      #     . venv/bin/activate
      #     . environments/.stackhpc/activate
      #     cd packer/
      #     packer init
      #     PACKER_LOG=1 packer build -except openstack.fatimage -on-error=ask -var-file=$PKR_VAR_environment_root/builder.pkrvars.hcl openstack.pkr.hcl
      #     ../dev/output_manifest.py packer-manifest.json # Sets NEW_COMPUTE_IMAGE_ID outputs

      # - name: Test reimage of compute nodes to new environment-specific image (via slurm)
      #   run: |
      #     . venv/bin/activate
      #     . environments/.stackhpc/activate
      #     ansible login -v -a "sudo scontrol reboot ASAP nextstate=RESUME reason='rebuild image:${{ steps.packer_build.outputs.NEW_COMPUTE_IMAGE_ID }}' ${TF_VAR_cluster_name}-compute-[0-3]"
      #     ansible compute -m wait_for_connection -a 'delay=60 timeout=600' # delay allows node to go down
      #     ansible-playbook -v ansible/ci/check_slurm.yml
      
      - name: Test reimage of login and control nodes (via rebuild adhoc)
        run: |
          . venv/bin/activate
          ansible-playbook -v --limit control,login ansible/adhoc/rebuild.yml
          ansible all -m wait_for_connection -a 'delay=60 timeout=600' # delay allows node to go down
          rm environments/.caas/inventory/cluster_hosts.yml
          ansible-playbook -v ansible/site.yml
          ansible-playbook -v ansible/ci/check_slurm.yml
      
      - name: Check sacct state survived reimage
        run: |
          . venv/bin/activate
          ansible-playbook -vv ansible/ci/check_sacct_hpctests.yml

      - name: Check MPI-based tests are shown in Grafana
        run: |
          . venv/bin/activate
          ansible-playbook -vv ansible/ci/check_grafana.yml

      - name: Delete infrastructure
        run: |
          . venv/bin/activate
          rm -f environments/.caas/inventory/cluster_hosts.yml
          ansible-playbook -v ansible/site.yml -e cluster_state=absent
        if: ${{ always() }} # success, failure or cancelled

      # - name: Delete images
      #   run: |
      #     . venv/bin/activate
      #     . environments/.stackhpc/activate
      #     ansible-playbook -vv ansible/ci/delete_images.yml
