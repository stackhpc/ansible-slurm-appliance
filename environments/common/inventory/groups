[login]
# All Slurm login nodes. Combined control/login nodes are not supported.

[control]
# A single Slurm control node. Multiple (high availability) control nodes are not supported.

[compute]
# All Slurm compute nodes (in all partitions).

[openhpc:children]
# All Slurm nodes.
login
control
compute

[hpctests]
# Login group to use for running mpi-based testing.

[cluster:children]
# All nodes in the appliance - add e.g. service nodes not running Slurm here.
openhpc

[builder]
# Do not add hosts here manually - used as part of Packer image build pipeline. See packer/README.md.

[podman:children]
# Hosts running containers for below services:
opendistro
filebeat

[prometheus]
# Single node to host monitoring server.

[grafana]
# Single node to host monitoring dashboards.

[alertmanager]
# TODO:

[opendistro]
# Single node to host ElasticSearch search engine for Slurm monitoring.

[slurm_stats]
# Single node to run tools to integrate Slurm's accounting information with ElasticSearch.
# NB: Host must be in `openhpc` group (for `sacct` command) and `opendistro` group.

[filebeat]
# Single node to parses log files for ElasticSearch - must be co-located with `slurm_stats`.

[nfs]
# All nodes which are appliance-controlled NFS servers or clients.

[mysql]
# Single node to run database used for Slurm accounting.

[node_exporter]
# All hosts to monitor for hardware and OS metrics.

[openhpc_tests:children]
# For post-deploy MPI-based tests - see ansible/adhoc/test.yml
login
compute

[selinux:children]
# All hosts requiring control of SELinux status.
cluster

[rebuild]
# Enable rebuild of nodes on an OpenStack cloud; add 'control' group plus 'compute' group or a subset of it.

[update]
# All hosts to (optionally) run yum update on.

[autoscale]
# Add control to enable autoscaling on OpenStack.
# See ansible/collections/ansible_collections/stackhpc/slurm_openstack_tools/roles/autoscale/README.md

[block_devices]
# Superset of hosts to configure filesystems on - see ansible/roles/block_devices/README.md

[basic_users]
# Add `openhpc` group to add slurm users via creation of users on each node.
