# This file
# 1. Ensures all groups in the appliance are always defined - even if empty
# 2. Defines dependencies between groups - child groups require & enables parent
#
# IMPORTANT
# ---------
# All groups and child groups here MUST be empty, as other environments cannot
# remove hosts/groups.

[login]
# All Slurm login nodes. Combined control/login nodes are not supported.

[control]
# A single Slurm control node. Multiple (high availability) control nodes are not supported.

[compute]
# All Slurm compute nodes (in all partitions).

[openhpc:children]
# All Slurm nodes.
login
control
compute

[hpctests:children]
# Login group to use for running mpi-based testing.
login

[additional]
# Additional nodes to include in "cluster" group
# Automatically populated from OpenTofu variable additional_nodegroups

[cluster:children]
# All nodes in the appliance - add e.g. service nodes not running Slurm here.
openhpc
additional

[builder]
# Do not add hosts here manually - used as part of Packer image build pipeline. See packer/README.md.

[topology]
# Compute nodes to be included in the Slurm topology plugin's topology tree. See ansible/roles/topology
# Should be set to `compute` if enabled
# Note that this feature currently assumes all compute nodes are VMs, enabling
# when the cluster contains baremetal compute nodes may lead to unexpected scheduling behaviour

[podman:children]
# Hosts running containers for below services:
opensearch
filebeat
mysql

[prometheus]
# Single node to host monitoring server.

[grafana]
# Single node to host monitoring dashboards.

[alertmanager]
# Single node to host alertmanager

[opensearch]
# Single node to host ElasticSearch search engine for Slurm monitoring.

[slurm_stats]
# Single node to run tools to integrate Slurm's accounting information with ElasticSearch.
# NB: Host must be in `openhpc` group (for `sacct` command) and `opensearch` group.

[filebeat]
# Single node to parses log files for ElasticSearch - must be co-located with `slurm_stats`.

[nfs]
# All nodes which are appliance-controlled NFS servers or clients.

[mysql]
# Single node to run database used for Slurm accounting.

[node_exporter]
# All hosts to monitor for hardware and OS metrics.

[selinux:children]
# All hosts requiring control of SELinux status.
cluster

[rebuild]
# Enable rebuild of nodes on an OpenStack cloud; add 'control' group.

[update]
# All hosts to (optionally) run yum update on.

[fail2ban]
# Hosts to install fail2ban on to protect SSH - uses firewalld
# https://www.fail2ban.org/wiki/index.php/Main_Page

[firewalld:children]
# Hosts to install firewalld on - see ansible/roles/firewalld
fail2ban

[basic_users]
# Add `openhpc` group to add slurm users via creation of users on each node.

[openondemand]
# Host to run Open OnDemand server on - subset of login

[openondemand_desktop]
# Subset of compute to run a interactive desktops on via Open OnDemand

[openondemand_jupyter]
# Subset of compute to run a Jupyter Notebook servers on via Open OnDemand

[openondemand_rstudio]
# Subset of compute to run RStudio servers on via Open OnDemand

[openondemand_matlab]
# Subset of compute to run a MATLAB interactive desktop on via Open OnDemand

[openondemand_codeserver]
# Subset of compute to run a Codeserver VSCode instance on via Open OnDemand

[etc_hosts]
# Hosts to manage /etc/hosts e.g. if no internal DNS. See ansible/roles/etc_hosts/README.md

[systemd:children]
# Hosts to make systemd unit adjustments on
opensearch
grafana
control
prometheus

[freeipa_server]
# Hosts to be a FreeIPA server. **NB**: Intended only for test/development use. See ansible/roles/freeipa/README.md

[freeipa_client]
# Hosts to be a FreeIPA client. See ansible/roles/freeipa/README.md

[freeipa:children]
# Allows defining variables common to freeipa_server and _client
freeipa_server
freeipa_client

[doca]
# Add `builder` to install NVIDIA DOCA during image build

[cuda]
# Hosts to install NVIDIA CUDA on - see ansible/roles/cuda/README.md

[slurm_recompile]
# Hosts to recompile Slurm for - allows supporting Slurm autodetection method 'nvml'

[vgpu]
# Hosts where vGPU/MIG should be configured - see docs/mig.md

[eessi]
# Hosts on which EESSI stack should be configured

[resolv_conf]
# Allows defining nameservers in /etc/resolv.conf - see ansible/roles/resolv_conf/README.md

[proxy]
# Hosts to configure http/s proxies - see ansible/roles/proxy/README.md

[manila]
# Hosts to configure for manila fileshares

[persist_hostkeys]
# Hosts to persist hostkeys for across reimaging. NB: Requires appliances_state_dir on hosts.

[squid]
# Hosts to run squid proxy

[tuned]
# Hosts to run TuneD configuration

[ansible_init]
# Hosts to run linux-ansible-init

[sssd]
# Hosts to configure sssd on

[sshd]
# Hosts where the OpenSSH server daemon should be configured

[compute_init]
# EXPERIMENTAL: Compute hosts to enable joining cluster on boot on

[k3s:children]
# Hosts to run k3s server/agent
k3s_server
k3s_agent

[k3s_server]
# Hosts to run k3s server (should only be single node i.e control node)

[k3s_agent]
# Hosts to run k3s agent

[k9s]
# Hosts to install k9s on

[lustre]
# Hosts to run lustre client

[extra_packages]
# Hosts to install specified additional packages on

[dnf_repos:children]
# Hosts to replace system repos with Pulp repos
# Roles/groups listed here *always* do installs:
extra_packages
doca
# TODO: can't express: if cuda and builder, enable dnf_repos

[pulp_site]
# Add builder to this group to automatically sync pulp during image build

[cacerts]
# Hosts to configure CA certificates and trusts on

[chrony]
# Hosts where crony configuration is applied. See docs/chrony.md for more details.

[gateway]
# Add builder to this group to install gateway ansible-init playbook into image

[nhc]
# Hosts to configure for node health checks - either entire 'compute' group or empty

[pulp_server]
# Host to deploy a Pulp server on and sync with mirrors of upstream Ark repositories. Should be a group containing a single VM provisioned
# separately from the appliance. e.g
# pulp_host ansible_host=<VM-ip-address>
# Note the host name can't conflict with group names i.e can't be called `pulp` or `pulp_server`

[raid]
# Add `builder` to configure image for software raid

[fatimage]
# Add build VM into this group to enable all features with this as child
