[login]
# Slurm login nodes. Combined control/login nodes are not supported.

[control]
# A single Slurm control node. Multiple (high availability) control nodes are not supported.

[compute]
# All Slurm compute nodes (in all partitions).

[openhpc]
# All Slurm nodes

[cluster]
# All nodes in the appliance - this allows for e.g. specific service nodes not running Slurm.
# See default below.

[cluster:children]
login
control
compute

[builder]
# VM used to (optionally) build compute images - do not add hosts. See packer/README.md.

[podman:children]
# Hosts running containers for below services:
opendistro
kibana
filebeat

[prometheus]
# Monitoring server(s)

[grafana]
# Dashboard server(s)

[alertmanager]
# TODO:

[opendistro]
# ElasticSearch server, used for Slurm monitoring.

[kibana]
# TODO:

[slurm_stats]
# Runs tools to integrate Slurm's accounting information with ElasticSearch.
# NB: Host must be in `openhpc` group (for `sacct` command) and `opendistro` group.

[filebeat]
# TODO:

[nfs]
# NFS servers or clients.

[mysql]
# Runs database used for Slurm accounting.

[node_exporter]
# Monitor these hosts for hardware and OS metrics

[openhpc_tests:children]
# Run post-deploy MPI-based tests using ansible/adhoc/test.yml
login
compute

[selinux:children]
# Define selinux status for these
cluster

[rebuild]
# Enable rebuild of nodes on an OpenStack cloud; add 'control' group plus 'compute' group or a subset of it.

[update]
# Nodes to run yum update on
